{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a652737f-9990-459c-b406-b3da19c4b2ba",
   "metadata": {},
   "source": [
    "# Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80cd4a6-4a3b-4a83-8239-3ac3b00e0eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import joblib \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from textblob import TextBlob\n",
    "from gensim.models import Word2Vec\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44581b6-a24c-4aff-b9c7-013c7dd048ea",
   "metadata": {},
   "source": [
    "# Data Extration from Excel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52397d8-793e-4bbd-859c-3e0d13792c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"final_data_2.xlsx\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0fca30-3bcd-44d8-8b9a-5c5e1ec998ff",
   "metadata": {},
   "source": [
    "# Model Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022a280d-09f4-4709-83d5-b96e1812d781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    tokens = word_tokenize(text)  # Tokenize text\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Lemmatize tokens\n",
    "    return ' '.join(tokens)  # Rejoin tokens\n",
    "\n",
    "# Apply preprocessing\n",
    "df['Processed_Review'] = df['reviews.text'].apply(preprocess_text)\n",
    "\n",
    "# TF-IDF with bigrams and trigrams\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 3))\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(df['Processed_Review'])\n",
    "joblib.dump(tfidf_vectorizer, \"tfidf_vectorizer.pkl\")\n",
    "\n",
    "# Sentiment Analysis\n",
    "def get_sentiment(text):\n",
    "    return TextBlob(text).sentiment.polarity  # Returns value between -1 (negative) and 1 (positive)\n",
    "\n",
    "df['Sentiment'] = df['Processed_Review'].apply(get_sentiment)\n",
    "\n",
    "# Word Count and Character Count\n",
    "df['Word_Count'] = df['Processed_Review'].apply(lambda x: len(x.split()))\n",
    "df['Char_Count'] = df['Processed_Review'].apply(len)\n",
    "\n",
    "# Dimensionality Reduction\n",
    "n_components = min(100, tfidf_features.shape[1])  # Adjust based on available features\n",
    "svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "reduced_features = svd.fit_transform(tfidf_features)\n",
    "joblib.dump(svd, \"truncated_svd.pkl\")\n",
    "# Convert to DataFrame\n",
    "reduced_df = pd.DataFrame(reduced_features, columns=[f'SVD_Component_{i+1}' for i in range(n_components)])\n",
    "\n",
    "# Select Additional Features\n",
    "df_select = df[['Sentiment', 'Word_Count', 'Char_Count']]\n",
    "model_final_features = pd.concat([df_select, reduced_df], axis=1)\n",
    "\n",
    "# Define features and target\n",
    "X = model_final_features\n",
    "y = df['Label']  # Ensure 'Label' column exists\n",
    "\n",
    "# Handle class imbalance using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_resampled_scaled = scaler.fit_transform(X_resampled)  # Scale first!\n",
    "joblib.dump(scaler, \"scaler_MinMax.pkl\")\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_resampled)\n",
    "joblib.dump(label_encoder,\"label_encoder.pkl\")\n",
    "\n",
    "# Now apply feature selection\n",
    "selector = SelectKBest(chi2, k=80)\n",
    "X_selected = selector.fit_transform(X_resampled_scaled, y_encoded)\n",
    "joblib.dump(selector,\"Select_features.pkl\")\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
    "\n",
    "# # Apply feature selection after scaling\n",
    "# selector = SelectKBest(chi2, k=80)  # Select top 80 features\n",
    "# X_selected = selector.fit_transform(X_resampled_scaled, y_resampled)\n",
    "\n",
    "# # Train-test split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_selected, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)\n",
    "\n",
    "# # Standardization\n",
    "\n",
    "\n",
    "# ?Train XGBoost\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict using both models\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "\n",
    "# ?Evaluate Models\n",
    "print(\"\\nXGBoost Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
    "\n",
    "print(\"\\nConfusion Matrix (XGB):\")\n",
    "print(confusion_matrix(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf118026-19d9-4fe5-811d-59401f210af3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
